#
# MIT License
#
# (C) Copyright 2021-2022 Hewlett Packard Enterprise Development LP
#
# Permission is hereby granted, free of charge, to any person obtaining a
# copy of this software and associated documentation files (the "Software"),
# to deal in the Software without restriction, including without limitation
# the rights to use, copy, modify, merge, publish, distribute, sublicense,
# and/or sell copies of the Software, and to permit persons to whom the
# Software is furnished to do so, subject to the following conditions:
#
# The above copyright notice and this permission notice shall be included
# in all copies or substantial portions of the Software.
#
# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL
# THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR
# OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,
# ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR
# OTHER DEALINGS IN THE SOFTWARE.
#
# Please refer to https://github.com/Cray-HPE/base-charts/blob/master/kubernetes/cray-service/values.yaml
# for more info on values you can set/override
# Note that cray-service.containers[*].image and cray-service.initContainers[*].image map values are one of the only structures that
# differ from the standard kubernetes container spec:
# image:
#   repository: ""
#   tag: "" (default = "latest")
#   pullPolicy: "" (default = "IfNotPresent")

nmn_tftp_service_ip: "10.92.100.60"
hmn_tftp_service_ip: "10.94.100.60"

cray-service:
  type: Deployment
  nameOverride: cray-tftpd-ipxe
  serviceAccountName: cray-ipxe
  priorityClassName: csm-high-priority-service
  podAnnotations:
    sidecar.istio.io/inject: "false"
  affinity:
    podAntiAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
      - labelSelector:
          matchExpressions:
          - key: app
            operator: In
            values:
            - cray-tftpd-ipxe
        topologyKey: "kubernetes.io/hostname"
  replicaCount: 3
  containers:
    - name: cray-tftp
      securityContext:
        runAsUser: 0
        runAsGroup: 0
        readOnlyRootFilesystem: false
        runAsNonRoot: false
      image:
        repository: artifactory.algol60.net/csm-docker/stable/cray-tftpd
      volumeMounts:
        - name: cray-tftp-data
          mountPath: /var/lib/tftpboot
          subPath: tftp
          readOnly: true
      livenessProbe:
        exec:
          command:
          - /app/tftp_liveness.sh
        initialDelaySeconds: 5
        periodSeconds: 30
      readinessProbe:
        exec:
          command:
          - stat
          - /var/lib/tftpboot/ipxe.efi
        initialDelaySeconds: 5
        periodSeconds: 10
    - name: cray-ipxe
      image:
        repository: artifactory.algol60.net/csm-docker/stable/cray-bss-ipxe
        tag: IMAGE_CRAY_BSS_IPXE_TAG
      volumeMounts:
      - name: cray-tftp-data
        mountPath: /shared_tftp
        subPath: tftp
      - name: client-auth
        mountPath: /client_auth
        readOnly: true
      - name: ca-public-key
        mountPath: /ca_public_key
      env:
      - name: IPXE_BUILD_TIME_LIMIT
        value: "40"
      - name: DEBUG_IPXE_BUILD_TIME_LIMIT
        value: "120"
      livenessProbe:
        exec:
          command:
          - python3
          - "-m"
          - crayipxe.liveness
        initialDelaySeconds: 40
        periodSeconds: 40

  volumes:
    - name: cray-tftp-data
      emptyDir: {}
    - name: client-auth
      secret:
        secretName: admin-client-auth
    - name: ca-public-key
      configMap:
        name: cray-configmap-ca-public-key
  restartPolicy: Always
  metallb-address-pool: node-management
  ingress:
    enabled: false
  service:
    enabled: false

ipxe:
  name: cray-ipxe
  namespace: services
  image_version: "latest"

  # Override with networks['node_management'].api_gw_service_dnsname
  api_gw: api-gw-service-nmn.local

  # The following are options that pertain to the build environment; checking in
  # values here change the resultant number of build environments that are created
  # as a response to updates or changes in the environment.
  build_bss: true
  build_shell: true
  chain_timeout: 10000

  # The following are options that pertain to which architectures are built for
  # the ipxe flavors that are listed above.
  build_x86: true

  # The following are options that pertain to how the ipxe binaries are built with
  # relation to embedding public CA certificates into images.
  build_with_cert: true

  # The following are options that pertain to the resultant interfaces that are
  # allowed as booting devices. By default, the resultant ipxe binary will instruct
  # nodes to boot over interfaces 2, 0, and 1, in that order. iPXE discovers and
  # labels interfaces based on how they're discovered based on their PCI id;
  # as a result, effective networks are attempted one after the other, until a
  # valid response from the boot script service (BSS) is successful. For systems
  # with homongenous hardware, it may be possible to alter the order of, or remove
  # networks which do not correspond to networks which expose boot services.
  # Overall time incurred on network miss depends on dhcp configuration on the
  # network attached to the NICs, but is about 10 seconds per failed network try.
  nic_boot_order:
    - net2
    - net0
    - net1
    - net3
    - net4
    - net5

  bss_max_attempts: 1024

  # The longest period of time to wait between attempts for contacting BSS over NICs
  # interated in cray_nic-boot_order. This number is effectively influenced by the
  # number of nodes expected to be booted simultaneously, and the effective number
  # of requests per second that BSS can serve out, assuming an average number of
  # requests per second. The exact number of requests per second is probably not
  # going to be uniform unless ramp rate limiting through capmc is used, or other
  # rate limiting mechanism with warmboot.
  #
  # This default value here is assuming a 250,0000 compute node system and a
  # throughput rate of BSS of about 4000 requests/second. This value is subject to
  # the number of BSS instances that are scaled, as well as the underlying datastore
  # of this service. These are read requests, so it is expected that scaling the number
  # of etcd replicas will improve overall throughput of BSS. As such, this value
  # is expected to change based on improvements to underlying microservices, and
  # can also be tuned based on the actual system size. In either case, this number
  # represents the pathological maximum to boot; in reality nothing should ever get
  # this high.
  bss_ceiling: 64
